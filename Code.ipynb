{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up the enviroment"
      ],
      "metadata": {
        "id": "uGaVadZHrmaP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GH_Nww0WN51Y",
        "outputId": "80f2d2f6-677e-4050-bf6e-d124429b0217"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting conllu\n",
            "  Downloading conllu-6.0.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading conllu-6.0.0-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: conllu\n",
            "Successfully installed conllu-6.0.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install conllu\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yXtGgC5gYfuh"
      },
      "outputs": [],
      "source": [
        "from conllu import parse\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from random import shuffle,randint\n",
        "from google.colab import drive\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from sklearn.metrics.cluster import homogeneity_score, completeness_score, v_measure_score\n",
        "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
        "from transformers import AutoTokenizer, BertModel\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connecting with my google drive to save and load trained weights, embeddings, and kmeans results.\n",
        "drive.mount('/content/drive')\n",
        "uposPath=\"/content/drive/MyDrive/L390.3/uposHMM/\"\n",
        "xposPath=\"/content/drive/MyDrive/L390.3/xposHMM/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsPtd--cp7oc",
        "outputId": "e63ab0a7-f9d8-4495-acbe-a729164c5527"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgZgLHGTKH_Q"
      },
      "source": [
        "# Preprocessing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BymWakXkOomK"
      },
      "outputs": [],
      "source": [
        "# Read the file content\n",
        "with open(\"/content/drive/MyDrive/L390.3/ptb-train.conllu\", \"r\", encoding=\"utf-8\") as file:\n",
        "    data = file.read()\n",
        "\n",
        "# Parse the file content\n",
        "sentences = parse(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBx5KADxO-Yc",
        "outputId": "eb56ed53-11a5-4bac-aeb6-5e930aa3e081"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences in the dataset = 39832\n",
            "Total number of words in the sentences = 950028\n",
            "Number of unique words = 44389\n",
            "Sentence average length = 23.850873669411527\n",
            "Length of the shortest sentence = 1\n",
            "Length of the longest sentence = 141\n",
            "Number of unique universial part of speech tags = 17\n",
            "Number of unique language-specific part of speech tags = 45\n",
            "How does an unlabelled sentence look like ['#', '200', 'million', 'of', 'undated', 'variable-rate', 'notes', 'priced', 'at', 'par', 'via', 'Merill', 'Lynch', 'International', 'Ltd', '.']\n",
            "How does a sentence look like in the processed Corpus [{'word': '#', 'tag': 'SYM'}, {'word': '200', 'tag': 'NUM'}, {'word': 'million', 'tag': 'NUM'}, {'word': 'of', 'tag': 'ADP'}, {'word': 'undated', 'tag': 'ADJ'}, {'word': 'variable-rate', 'tag': 'ADJ'}, {'word': 'notes', 'tag': 'NOUN'}, {'word': 'priced', 'tag': 'VERB'}, {'word': 'at', 'tag': 'ADP'}, {'word': 'par', 'tag': 'ADJ'}, {'word': 'via', 'tag': 'ADP'}, {'word': 'Merill', 'tag': 'PROPN'}, {'word': 'Lynch', 'tag': 'PROPN'}, {'word': 'International', 'tag': 'PROPN'}, {'word': 'Ltd', 'tag': 'PROPN'}, {'word': '.', 'tag': 'PUNCT'}]\n"
          ]
        }
      ],
      "source": [
        "upos=set() # all unique universial part of speech tags\n",
        "xpos=set() # all unique language specific part of speech tags\n",
        "vocab=set() # all unique words\n",
        "uposCorpus=[] # List of sentences where each sentence is composed of words and their universial part of speech tags\n",
        "xposCorpus=[] # List of sentences where each sentence is composed of words and their language specific part of speech tags\n",
        "allUnlabeldSentences = []\n",
        "num_words=0\n",
        "longestSentenceLength = 0\n",
        "shortestSentenceLength = float(\"inf\")\n",
        "for s in sentences:\n",
        "  longestSentenceLength=max(longestSentenceLength,len(s))\n",
        "  shortestSentenceLength = min(shortestSentenceLength,len(s))\n",
        "  num_words+=len(s)\n",
        "  uposSentence=[]\n",
        "  xposSentence=[]\n",
        "  unlabeledSentence=[]\n",
        "  for token in s:\n",
        "    word=token[\"form\"]\n",
        "    uposTag=token[\"upos\"]\n",
        "    xposTag=token[\"xpos\"]\n",
        "    upos.add(uposTag)\n",
        "    xpos.add(xposTag)\n",
        "    vocab.add(word)\n",
        "    unlabeledSentence.append(word)\n",
        "    uposSentence.append({\"word\":word,\"tag\":uposTag})\n",
        "    xposSentence.append({\"word\":word,\"tag\":xposTag})\n",
        "  allUnlabeldSentences.append(unlabeledSentence)\n",
        "  uposCorpus.append(uposSentence)\n",
        "  xposCorpus.append(xposSentence)\n",
        "allUnlabeldSentences.sort()\n",
        "uposCorpus.sort(key=lambda a: [word[\"word\"] for word in a])\n",
        "xposCorpus.sort(key=lambda a: [word[\"word\"] for word in a])\n",
        "upos=sorted(upos)\n",
        "xpos=sorted(xpos)\n",
        "vocab=sorted(vocab)\n",
        "wordToIndex = {vocab[i] : i for i in range(len(vocab))}\n",
        "uposTagToIndex = {upos[i] : i for i in range(len(upos))}\n",
        "xposTagToIndex = {xpos[i] : i for i in range(len(xpos))}\n",
        "print(f\"Number of sentences in the dataset = {len(sentences)}\")\n",
        "print(f\"Total number of words in the sentences = {num_words}\")\n",
        "print(f\"Number of unique words = {len(vocab)}\")\n",
        "print(f\"Sentence average length = {num_words/len(sentences)}\")\n",
        "print(f\"Length of the shortest sentence = {shortestSentenceLength}\")\n",
        "print(f\"Length of the longest sentence = {longestSentenceLength}\")\n",
        "print(f\"Number of unique universial part of speech tags = {len(upos)}\")\n",
        "print(f\"Number of unique language-specific part of speech tags = {len(xpos)}\")\n",
        "print(f\"How does an unlabelled sentence look like {allUnlabeldSentences[0]}\")\n",
        "print(f\"How does a sentence look like in the processed Corpus {uposCorpus[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckDpVkl2UFXd",
        "outputId": "ced0c19a-8205-48b7-c969-5263bbe1aa85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of sentences in the testCorpus = 10000\n",
            "number of unique tags in the testCorpus = 17\n",
            "number of unique words in the testCorpus = 22061\n"
          ]
        }
      ],
      "source": [
        "# Creating a smaller dataset to test the model on and estimate the best hyperparmeters\n",
        "# The code on this cell was only used on the development process. For the final models I used the whole datasets.\n",
        "testCorpus=uposCorpus[:10000]\n",
        "testTags=set()\n",
        "testVocab=set()\n",
        "for s in testCorpus:\n",
        "  for word in s:\n",
        "    testTags.add(word[\"tag\"])\n",
        "    testVocab.add(word[\"word\"])\n",
        "testTags=list(testTags)\n",
        "testVocab=list(testVocab)\n",
        "\n",
        "testUnlabeldSentences=[]\n",
        "for sentenceWithTag in testCorpus:\n",
        "  s=[word[\"word\"] for word in sentenceWithTag]\n",
        "  testUnlabeldSentences.append(s)\n",
        "\n",
        "print(f\"number of sentences in the testCorpus = {len(testCorpus)}\")\n",
        "print(f\"number of unique tags in the testCorpus = {len(testTags)}\")\n",
        "print(f\"number of unique words in the testCorpus = {len(testVocab)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HMM Models\n",
        "\n",
        "## I trained 4 HMM models.\n",
        "\n",
        "### 1-HMM model trained on the universal part of speech corpus using a supervised approch.\n",
        "### 2-HMM model trained on the language-specific part of speech corpus using a supervised approch.\n",
        "### 3-HMM model trained on the universal part of speech corpus using an unsupervised approach (Forward Backward algorithim).\n",
        "### 4-HMM model trained on the language-specific part of speech corpus using an unsupervised approach (Forward Backward algorithim)."
      ],
      "metadata": {
        "id": "Hmza-MyXvqhO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TQofcz1eVnLI"
      },
      "outputs": [],
      "source": [
        "SMALLEST_REPRESENTABLE_FLOAT=np.finfo(np.float64).tiny\n",
        "\n",
        "class HMM:\n",
        "  # all probabilities are in log space to avoid underflow\n",
        "  def __init__(self,hiddenStates,startFromScratch=True):\n",
        "    self.hiddenStates = hiddenStates\n",
        "    self.stateToIndex = {self.hiddenStates[i] : i for i in range(len(self.hiddenStates))}\n",
        "    if startFromScratch:\n",
        "      self.__initialize_startProbability_randomly()\n",
        "      self.__initialize_transitionMatrix_randomly()\n",
        "      self.__initialize_emissionMatrix_randomly()\n",
        "    else:\n",
        "      # Load the previously trained parameters\n",
        "      path = uposPath if len(self.hiddenStates)==17 else xposPath\n",
        "      self.startProbability = np.load(path + \"startProbability.npy\")\n",
        "      self.transitionMatrix = np.load(path + \"transitionMatrix.npy\")\n",
        "      self.emissionMatrix = np.load(path + \"emissionMatrix.npy\")\n",
        "\n",
        "  def __logSpaceAdd(self,numpyArray,axis=None):\n",
        "    # I am using the log-sum-exp trick\n",
        "    if axis==None:\n",
        "      if numpyArray.shape[0]==1:\n",
        "        axis=1\n",
        "      elif numpyArray.shape[1]==1:\n",
        "        axis=0\n",
        "      else:\n",
        "        raise Exception(\"Axis was not specified\")\n",
        "    maxValue=np.max(numpyArray,axis=axis,keepdims=True)\n",
        "    temp = np.log(np.sum(np.exp(numpyArray - maxValue),axis=axis,keepdims=True))\n",
        "    ans=maxValue + temp\n",
        "    if ans.shape==(1,1):\n",
        "      ans=ans[0][0]\n",
        "    return ans\n",
        "\n",
        "  def __logSpaceAdd2Columns(self,col1,col2):\n",
        "    ans=np.column_stack((col1,col2))\n",
        "    return self.__logSpaceAdd(ans,axis=1)\n",
        "\n",
        "  def __logSpaceAdd2Matricies(self,mat1,mat2):\n",
        "    ans=self.__logSpaceAdd(np.stack((mat1,mat2),axis=0),axis=0)\n",
        "    return ans.reshape(ans.shape[1:])\n",
        "\n",
        "\n",
        "  def __initialize_startProbability_randomly(self):\n",
        "    self.startProbability = np.random.rand(len(self.hiddenStates),1)+1e-10\n",
        "    self.startProbability/=np.sum(self.startProbability)\n",
        "    self.startProbability=np.log(self.startProbability) # covert to log space\n",
        "    return\n",
        "\n",
        "  def __initialize_transitionMatrix_randomly(self):\n",
        "    self.transitionMatrix =  np.random.rand(len(self.hiddenStates),len(self.hiddenStates)) + 1e-10\n",
        "    for i in range(len(self.hiddenStates)):\n",
        "      self.transitionMatrix[i]/=np.sum(self.transitionMatrix[i])\n",
        "    self.transitionMatrix=np.log(self.transitionMatrix) # covert to log space\n",
        "    return\n",
        "\n",
        "  def __initialize_emissionMatrix_randomly(self):\n",
        "    self.emissionMatrix = np.random.rand(len(self.hiddenStates),len(vocab)) + 1e-10\n",
        "    for i in range(len(self.hiddenStates)):\n",
        "      self.emissionMatrix[i]/=np.sum(self.emissionMatrix[i])\n",
        "    self.emissionMatrix=np.log(self.emissionMatrix) # covert to log space\n",
        "    return\n",
        "\n",
        "  def viterbiAlgorithm(self,sentence):\n",
        "    # multiplication is addition in log space\n",
        "    numTimeSteps,numHiddenStates=len(sentence),len(self.hiddenStates)\n",
        "    viterbi=np.zeros((numHiddenStates,numTimeSteps))\n",
        "    backPointer=np.full((numHiddenStates,numTimeSteps),-1)\n",
        "    firstWordIndex=wordToIndex[sentence[0]]\n",
        "    viterbi[:,0:1] = self.startProbability + self.emissionMatrix[:,firstWordIndex:firstWordIndex+1]\n",
        "    for timeStep in range(1,numTimeSteps):\n",
        "      wordIndex=wordToIndex[sentence[timeStep]]\n",
        "      viterbi[:,timeStep:timeStep+1] = np.max(viterbi[:,timeStep-1:timeStep] + self.transitionMatrix, axis=0, keepdims=True).T + self.emissionMatrix[:,wordIndex:wordIndex+1]\n",
        "      backPointer[:,timeStep:timeStep+1] = np.argmax(viterbi[:,timeStep-1:timeStep] + self.transitionMatrix, axis=0, keepdims=True).T\n",
        "\n",
        "    maxProbability , lastState = np.max(viterbi[:,numTimeSteps-1:numTimeSteps]),np.argmax(viterbi[:,numTimeSteps-1:numTimeSteps])\n",
        "    mostProbableSequence,currentState,currentTimeStep = [], lastState,numTimeSteps-1\n",
        "    while currentState != -1:\n",
        "      mostProbableSequence.append(currentState) # we are appendeing the index of the state\n",
        "      currentState=backPointer[currentState][currentTimeStep]\n",
        "      currentTimeStep-=1\n",
        "    mostProbableSequence.reverse()\n",
        "    return mostProbableSequence\n",
        "\n",
        "  def forwardAlgorithm(self,sentence):\n",
        "    # multiplication is addition in log space\n",
        "    # for addition in log space I am using the log-sum-exp trick\n",
        "    numTimeSteps,numHiddenStates=len(sentence),len(self.hiddenStates)\n",
        "    forward=np.zeros((numHiddenStates,numTimeSteps)) # forward[i][t] the joint probability to see word(1),word(2),..word(t) and state(t) = i.\n",
        "    firstWordIndex=wordToIndex[sentence[0]]\n",
        "    forward[:,0:1] = self.startProbability + self.emissionMatrix[: ,firstWordIndex:firstWordIndex+1]\n",
        "    for timeStep in range(1,numTimeSteps):\n",
        "      wordIndex=wordToIndex[sentence[timeStep]]\n",
        "      forward[:,timeStep:timeStep+1] = self.__logSpaceAdd(forward[:,timeStep-1:timeStep] + self.transitionMatrix, axis=0).T + self.emissionMatrix[:,wordIndex:wordIndex+1]\n",
        "    forwardProbability=self.__logSpaceAdd(forward[:,numTimeSteps-1:numTimeSteps])\n",
        "    return forwardProbability,forward\n",
        "\n",
        "  def backwardAlgorithm(self,sentence):\n",
        "    # multiplication is addition in log space\n",
        "    # for addition in log space I am using the log-sum-exp trick\n",
        "    numTimeSteps,numHiddenStates=len(sentence),len(self.hiddenStates)\n",
        "    backward=np.zeros((numHiddenStates,numTimeSteps)) # backward[i][t] the joint probability to see word(t+1),word(t+2),..word(numTimeSteps-1) and state(t) = i.\n",
        "    backward[:,numTimeSteps-1:numTimeSteps]=np.zeros((numHiddenStates,1)) # log(1) = 0; these values should be 1 but we are opreating in log space\n",
        "    for timeStep in range(numTimeSteps-2,-1,-1):\n",
        "      wordIndex=wordToIndex[sentence[timeStep+1]]\n",
        "      backward[:,timeStep:timeStep+1] = self.__logSpaceAdd((backward[:,timeStep+1:timeStep+2] + self.emissionMatrix[:,wordIndex:wordIndex+1]).T + self.transitionMatrix, axis=1)\n",
        "    firstWordIndex=wordToIndex[sentence[0]]\n",
        "    backwardProbability= self.__logSpaceAdd(self.startProbability+self.emissionMatrix[:,firstWordIndex:firstWordIndex+1]+backward[:,0:1])\n",
        "    return backwardProbability,backward\n",
        "\n",
        "  def forwardBackwardAlgorithm(self, batch):\n",
        "    # multiplication is addition in log space\n",
        "    # for addition in log space I am using the log-sum-exp trick\n",
        "\n",
        "    # estimation step\n",
        "    numHiddenStates,vocabSize=len(self.hiddenStates), len(vocab)\n",
        "    zeroInLogSpace = np.log(SMALLEST_REPRESENTABLE_FLOAT)\n",
        "    estimatedStateStartCount = np.full((numHiddenStates,1),zeroInLogSpace)\n",
        "    estimatedStateTransitionCount = np.full((numHiddenStates,numHiddenStates),zeroInLogSpace)\n",
        "    estimatedStateEmissionCount = np.full((numHiddenStates,vocabSize),zeroInLogSpace)\n",
        "\n",
        "    for sentence in batch:\n",
        "      numTimeSteps=len(sentence)\n",
        "      sentenceProbability,forward=self.forwardAlgorithm(sentence)\n",
        "      sentenceProbability,backward=self.backwardAlgorithm(sentence)\n",
        "\n",
        "\n",
        "      # estimating the state start count\n",
        "      firstWordIndex=wordToIndex[sentence[0]]\n",
        "      current_estimated_count = (self.startProbability + self.emissionMatrix[:, firstWordIndex:firstWordIndex+1] + backward[:, 0:1]) - sentenceProbability\n",
        "      estimatedStateStartCount = self.__logSpaceAdd2Columns(estimatedStateStartCount,current_estimated_count)\n",
        "\n",
        "      for timeStep in range(numTimeSteps-1):\n",
        "        # estimating the state transitions count\n",
        "        wordIndex=wordToIndex[sentence[timeStep+1]]\n",
        "        current_estimated_count = (((forward[:,timeStep:timeStep+1] + self.transitionMatrix) + self.emissionMatrix[:,wordIndex:wordIndex+1].T) + backward[:,timeStep+1:timeStep+2].T) - sentenceProbability\n",
        "        estimatedStateTransitionCount = self.__logSpaceAdd2Matricies(estimatedStateTransitionCount,current_estimated_count)\n",
        "\n",
        "      for timeStep in range(numTimeSteps):\n",
        "        # estimating the state emission count\n",
        "        wordIndex = wordToIndex[sentence[timeStep]]\n",
        "        current_estimated_count = forward[:,timeStep:timeStep+1] + backward[:,timeStep:timeStep+1] - sentenceProbability\n",
        "        estimatedStateEmissionCount[:,wordIndex:wordIndex+1] = self.__logSpaceAdd2Columns(estimatedStateEmissionCount[:,wordIndex:wordIndex+1],current_estimated_count)\n",
        "\n",
        "    # maxmization step\n",
        "\n",
        "    # calculating startprobability matrix\n",
        "    self.startProbability=estimatedStateStartCount-self.__logSpaceAdd(estimatedStateStartCount)\n",
        "    # calculating transition matrix\n",
        "    self.transitionMatrix = estimatedStateTransitionCount-self.__logSpaceAdd(estimatedStateTransitionCount,axis=1)\n",
        "    # calculating emission matrix\n",
        "    self.emissionMatrix = estimatedStateEmissionCount-self.__logSpaceAdd(estimatedStateEmissionCount,axis=1)\n",
        "    return\n",
        "\n",
        "  def unsupervisedTraining(self,numEpochs,batchSize,sentences):\n",
        "    for epoch in range(numEpochs):\n",
        "      for i in range(0,len(sentences),batchSize):\n",
        "        currentBatch=sentences[i:min(i+batchSize,len(sentences))]\n",
        "        self.forwardBackwardAlgorithm(currentBatch)\n",
        "      print(f\"Finished Epoch Number {epoch+1}\")\n",
        "      # save the model parameters after each epoch\n",
        "      path = uposPath if len(self.hiddenStates)==17 else xposPath\n",
        "      np.save(path + \"startProbability.npy\",self.startProbability)\n",
        "      np.save(path + \"transitionMatrix.npy\",self.transitionMatrix)\n",
        "      np.save(path + \"emissionMatrix.npy\",self.emissionMatrix)\n",
        "    return\n",
        "\n",
        "  def supervisedTraining(self,corpus):\n",
        "    numHiddenStates , vocabSize= len(self.hiddenStates) , len(vocab)\n",
        "    state_i_to_state_j_cnt = [[0] * numHiddenStates for _ in range(numHiddenStates)]\n",
        "    state_start_cnt = [0] * numHiddenStates\n",
        "    state_to_word_cnt = [[0] * vocabSize for _ in range(numHiddenStates)]\n",
        "\n",
        "    for sentence in corpus:\n",
        "      startStateIndex=self.stateToIndex[sentence[0][\"tag\"]]\n",
        "      startWordIndex=wordToIndex[sentence[0][\"word\"]]\n",
        "      state_start_cnt[startStateIndex] += 1\n",
        "      state_to_word_cnt[startStateIndex][startWordIndex] += 1\n",
        "      for i in range(1,len(sentence)):\n",
        "        previousStateIndex=self.stateToIndex[sentence[i-1][\"tag\"]]\n",
        "        currentSateIndex=self.stateToIndex[sentence[i][\"tag\"]]\n",
        "        currentWordIndex=wordToIndex[sentence[i][\"word\"]]\n",
        "        state_i_to_state_j_cnt[previousStateIndex][currentSateIndex] += 1\n",
        "        state_to_word_cnt[currentSateIndex][currentWordIndex] +=1\n",
        "\n",
        "    # esitmating transition matrix\n",
        "    for state1 in range(numHiddenStates):\n",
        "      state1_cnt=sum(state_i_to_state_j_cnt[state1])\n",
        "      for state2 in range(numHiddenStates):\n",
        "        self.transitionMatrix[state1][state2] = state_i_to_state_j_cnt[state1][state2]/state1_cnt\n",
        "        if self.transitionMatrix[state1][state2]==0:\n",
        "          self.transitionMatrix[state1][state2]=SMALLEST_REPRESENTABLE_FLOAT\n",
        "\n",
        "    # estimating start probability\n",
        "    num_starts=sum(state_start_cnt)\n",
        "    for state in range(numHiddenStates):\n",
        "      self.startProbability[state]=state_start_cnt[state]/num_starts\n",
        "      if self.startProbability[state]==0:\n",
        "        self.startProbability[state]=SMALLEST_REPRESENTABLE_FLOAT\n",
        "\n",
        "    # estimating emission matrix\n",
        "    for state in range(numHiddenStates):\n",
        "      num_occurances=sum(state_to_word_cnt[state])\n",
        "      for word in range(vocabSize):\n",
        "        self.emissionMatrix[state][word]= state_to_word_cnt[state][word]/num_occurances\n",
        "        if self.emissionMatrix[state][word]==0:\n",
        "          self.emissionMatrix[state][word]=SMALLEST_REPRESENTABLE_FLOAT\n",
        "\n",
        "    # convert to log space\n",
        "    self.startProbability=np.log(self.startProbability)\n",
        "    self.transitionMatrix=np.log(self.transitionMatrix)\n",
        "    self.emissionMatrix=np.log(self.emissionMatrix)\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 1\n",
        "supervisedUposHmm=HMM(upos)\n",
        "supervisedUposHmm.supervisedTraining(uposCorpus)"
      ],
      "metadata": {
        "id": "ZapZ8lknei1o"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 2\n",
        "supervisedXposHmm=HMM(xpos)\n",
        "supervisedXposHmm.supervisedTraining(xposCorpus)"
      ],
      "metadata": {
        "id": "WqFY4KGY5CIl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 3\n",
        "numEpochs, batchSize=20, 512\n",
        "unsupervisedUposHmm=HMM(upos)\n",
        "unsupervisedUposHmm.unsupervisedTraining(numEpochs,batchSize,allUnlabeldSentences)\n",
        "unsupervisedUposHmm.unsupervisedTraining(numEpochs,2*batchSize,allUnlabeldSentences)\n",
        "unsupervisedUposHmm.unsupervisedTraining(numEpochs,2*2*batchSize,allUnlabeldSentences)\n"
      ],
      "metadata": {
        "id": "ZbtjEAzt5Ng2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 4\n",
        "numEpochs, batchSize= 20, 512\n",
        "unsupervisedXposHmm=HMM(xpos)\n",
        "unsupervisedXposHmm.unsupervisedTraining(numEpochs,batchSize,allUnlabeldSentences)\n",
        "unsupervisedXposHmm.unsupervisedTraining(numEpochs,2*batchSize,allUnlabeldSentences)\n",
        "unsupervisedXposHmm.unsupervisedTraining(numEpochs,2*2*batchSize,allUnlabeldSentences)"
      ],
      "metadata": {
        "id": "VupijB2q5yY8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-Means Clustring with BERT Embeddings\n",
        "\n",
        "## 1- K-Means on the Universal part of speech tags corpus\n",
        "\n",
        "## 2- K-Means on the Language specific part of speech tags corpus"
      ],
      "metadata": {
        "id": "lPhpvFk6g3wQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining BERT Model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "cjPV1I-Chx77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration (GPU/CPU)\n",
        "# GPU is recommended to get the embeddings (5 min on a GPU vs 4 hours on a CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"The current device is {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7W98-6ivnaE",
        "outputId": "af91e621-fdac-484d-d855-74a6dd3e0fef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The current device is cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the word embeddings\n",
        "wordEmbeddings = []\n",
        "sentences=[' '.join(s) for s in allUnlabeldSentences]\n",
        "batchSize=250\n",
        "sentenceIndex=0\n",
        "for startIndex in range(0,len(sentences),batchSize):\n",
        "    batch = sentences[startIndex:min(len(sentences),startIndex+batchSize)]\n",
        "    inputs = tokenizer(batch, padding=True,return_tensors=\"pt\",add_special_tokens=False)\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        # For this task I found out that the first layer embeddings are the best ones to use.\n",
        "        batch_hidden_states = outputs[2][1].cpu()\n",
        "\n",
        "    inputs = tokenizer(batch, padding=True,return_tensors=\"pt\",add_special_tokens=False)\n",
        "    for i in range(len(batch_hidden_states)):\n",
        "        originalWords = sentences[sentenceIndex].split()\n",
        "        tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][i])\n",
        "        tokenEmbeddings = batch_hidden_states[i]\n",
        "        tokenIndex=0\n",
        "        for word in originalWords:\n",
        "            word=word.lower()\n",
        "            constructedWord = \"\"\n",
        "            wordTokensEmbeddings = []\n",
        "            while constructedWord!=word:\n",
        "                currentToken = tokens[tokenIndex] if len(tokens[tokenIndex])<3 or tokens[tokenIndex][:2]!=\"##\" else tokens[tokenIndex][2:]\n",
        "                constructedWord += currentToken\n",
        "                wordTokensEmbeddings.append(tokenEmbeddings[tokenIndex])\n",
        "                tokenIndex+=1\n",
        "            wordEmbeddings.append(np.mean(wordTokensEmbeddings,axis=0))\n",
        "        sentenceIndex+=1\n",
        "np.save(\"/content/drive/MyDrive/L390.3/wordEmbeddings.npy\",wordEmbeddings)"
      ],
      "metadata": {
        "id": "jvVYeLZihEKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading pretrained embeddings.\n",
        "wordEmbeddings=np.load(\"/content/drive/MyDrive/L390.3/wordEmbeddings.npy\")"
      ],
      "metadata": {
        "id": "ZE595UJxHnBd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 5\n",
        "# training the upos kmeans model\n",
        "uposKmeans = KMeans(n_clusters=17, random_state=0) # I tried multiple random_state values and then choose the best one. You can do this automatically by changing the value of the parameter n_init (see https://scikit-learn.org/1.5/modules/generated/sklearn.cluster.KMeans.html).\n",
        "uposKmeans.fit(wordEmbeddings)\n",
        "uposKmeansPredictions = uposKmeans.labels_\n",
        "np.save(\"/content/drive/MyDrive/L390.3/uposKmeansPredictions.npy\",uposKmeansPredictions)\n",
        "print(\"Number of iterations:\", uposKmeans.n_iter_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPy4kV3UBYen",
        "outputId": "d6dc81be-3449-4137-9773-a24be874edd1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of iterations: 18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 6\n",
        "# training the xpos kmeans model\n",
        "xposKmeans = KMeans(n_clusters=45, random_state=2) # I tried multiple random_state values and then choose the best one. You can do this automatically by changing the value of the parameter n_init (see https://scikit-learn.org/1.5/modules/generated/sklearn.cluster.KMeans.html).\n",
        "xposKmeans.fit(wordEmbeddings)\n",
        "xposKmeansPredictions = xposKmeans.labels_\n",
        "np.save(\"/content/drive/MyDrive/L390.3/xposKmeansPredictions.npy\",xposKmeansPredictions)\n",
        "print(\"Number of iterations:\", xposKmeans.n_iter_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yg_ck-dViEx8",
        "outputId": "520d831f-722c-4b61-ada4-9ade0f1b178a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of iterations: 59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Measurment\n",
        "\n",
        "## I am using 3 main type of measurments to compare the performance of the models\n",
        "\n",
        "### 1- Word Level Accuracy\n",
        "### 2- V-measure\n",
        "### 3- Variation of information"
      ],
      "metadata": {
        "id": "EkvxZ1bzowrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the measurement functions\n",
        "def calculate_entropy(cluster):\n",
        "  \"\"\"Calculate the entropy of a clustering.\"\"\"\n",
        "  total_points = len(cluster)\n",
        "  if total_points == 0:\n",
        "      return 0\n",
        "  label_counts = Counter(cluster)\n",
        "  probabilities = [count / total_points for count in label_counts.values()]\n",
        "  entropy = -sum(p * np.log2(p) for p in probabilities)\n",
        "  return entropy\n",
        "\n",
        "def calculate_mutual_information(U, V):\n",
        "  \"\"\"Calculate the mutual information between two clusterings.\"\"\"\n",
        "  total_points = len(U)\n",
        "  mutual_info = 0\n",
        "  U_labels, V_labels = set(U), set(V)\n",
        "  for u in U_labels:\n",
        "      for v in V_labels:\n",
        "          intersection_size = sum(1 for i in range(total_points) if U[i] == u and V[i] == v)\n",
        "          if intersection_size == 0:\n",
        "              continue\n",
        "          p_u = sum(1 for x in U if x == u) / total_points\n",
        "          p_v = sum(1 for x in V if x == v) / total_points\n",
        "          p_uv = intersection_size / total_points\n",
        "          mutual_info += p_uv * np.log2(p_uv / (p_u * p_v))\n",
        "  return mutual_info\n",
        "\n",
        "# The 3 measures I am using\n",
        "\n",
        "def calculate_accuracy(goldStandard,prediction):\n",
        "  numTags=len(set(goldStandard))\n",
        "  confusionMatrix= np.zeros((numTags,numTags),dtype=int) # confusionMatrix[i][j] => if cluster i is mapped to tag with index j, how many correct tags will I get.\n",
        "  for i in range(len(prediction)):\n",
        "    confusionMatrix[prediction[i]][goldStandard[i]]+=1\n",
        "  _,clusterToTagIndex=linear_sum_assignment(-confusionMatrix) # Hungarian algorithm implementaion to find the optimal matching that maxmizes the accuracy.\n",
        "  prediction = [clusterToTagIndex[cluster] for cluster in prediction]\n",
        "  correct_words = sum([prediction[i]==goldStandard[i] for i in range(len(goldStandard))])\n",
        "  return 100 * correct_words/len(goldStandard)\n",
        "\n",
        "def calculate_v_measure(true_labels, predicted_labels):\n",
        "  homo_score = homogeneity_score(true_labels, predicted_labels)\n",
        "  comp_score = completeness_score(true_labels, predicted_labels)\n",
        "  v_score = v_measure_score(true_labels, predicted_labels)\n",
        "  return homo_score, comp_score, v_score\n",
        "\n",
        "def calculate_variation_of_information(U, V):\n",
        "  \"\"\"Calculate the variation of information between two clusterings.\"\"\"\n",
        "  entropy_U = calculate_entropy(U)\n",
        "  entropy_V = calculate_entropy(V)\n",
        "  mutual_information = calculate_mutual_information(U, V)\n",
        "  variation_of_information = entropy_U + entropy_V - 2 * mutual_information\n",
        "  return variation_of_information, variation_of_information / (entropy_U + entropy_V)"
      ],
      "metadata": {
        "id": "WZR1zuFIo0uI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To calculate the measures across different sentences, I concatnated the results for different sentences as if the corups is a one long sentence.\n",
        "# I decided to do this because the clusters are uniform across sentences and the task is to see how good the models are in clustring words, regardless of the sentence level performance.\n",
        "\n",
        "# Generating the Gold standards\n",
        "uposGoldStandard=[]\n",
        "for sentenceWithTag in uposCorpus:\n",
        "  uposGoldStandard.extend([uposTagToIndex[word[\"tag\"]] for word in sentenceWithTag])\n",
        "\n",
        "xposGoldStandard=[]\n",
        "for sentenceWithTag in xposCorpus:\n",
        "  xposGoldStandard.extend([xposTagToIndex[word[\"tag\"]] for word in sentenceWithTag])\n",
        "\n",
        "# Generating HMM models outputs\n",
        "supervisedUposHmmPredictions=[]\n",
        "supervisedXposHmmPredictions=[]\n",
        "unsupervisedUposHmmPredictions=[]\n",
        "unsupervisedXposHmmPredictions=[]\n",
        "for s in allUnlabeldSentences:\n",
        "  supervisedUposHmmPredictions.extend(supervisedUposHmm.viterbiAlgorithm(s))\n",
        "  supervisedXposHmmPredictions.extend(supervisedXposHmm.viterbiAlgorithm(s))\n",
        "  unsupervisedUposHmmPredictions.extend(unsupervisedUposHmm.viterbiAlgorithm(s))\n",
        "  unsupervisedXposHmmPredictions.extend(unsupervisedXposHmm.viterbiAlgorithm(s))\n",
        "\n",
        "# Loading Kmeans outputs\n",
        "uposKmeansPredictions = np.load(\"/content/drive/MyDrive/L390.3/uposKmeansPredictions.npy\")\n",
        "xposKmeansPredictions = np.load(\"/content/drive/MyDrive/L390.3/xposKmeansPredictions.npy\")\n"
      ],
      "metadata": {
        "id": "ADFG3LTbFoNs"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reporting the resuls of the models on the 3 measurments\n",
        "\n",
        "# supervisedUposHmm\n",
        "print(f\"The Word Level Accuracy of supervisedUposHmm = {calculate_accuracy(uposGoldStandard,supervisedUposHmmPredictions)} %\")\n",
        "print(f\"The v measure of supervisedUposHmm = {calculate_v_measure(uposGoldStandard,supervisedUposHmmPredictions)}\")\n",
        "print(f\"The variation of information of supervisedUposHmm = {calculate_variation_of_information(uposGoldStandard,supervisedUposHmmPredictions)}\\n\")\n",
        "\n",
        "# supervisedxposHmm\n",
        "print(f\"The Word Level Accuracy of supervisedxposHmm = {calculate_accuracy(xposGoldStandard,supervisedXposHmmPredictions)} %\")\n",
        "print(f\"The v measure of supervisedxposHmm = {calculate_v_measure(xposGoldStandard,supervisedXposHmmPredictions)}\")\n",
        "print(f\"The variation of information of supervisedxposHmm = {calculate_variation_of_information(xposGoldStandard,supervisedXposHmmPredictions)}\\n\")\n",
        "\n",
        "# unsupervisedUposHmm\n",
        "print(f\"The Word Level Accuracy of unsupervisedUposHmm = {calculate_accuracy(uposGoldStandard,unsupervisedUposHmmPredictions)} %\")\n",
        "print(f\"The v measure of unsupervisedUposHmm = {calculate_v_measure(uposGoldStandard,unsupervisedUposHmmPredictions)}\")\n",
        "print(f\"The variation of information of unsupervisedUposHmm = {calculate_variation_of_information(uposGoldStandard,unsupervisedUposHmmPredictions)}\\n\")\n",
        "\n",
        "# unsupervisedXposHmm\n",
        "print(f\"The Word Level Accuracy of unsupervisedXposHmm = {calculate_accuracy(xposGoldStandard,unsupervisedXposHmmPredictions)} %\")\n",
        "print(f\"The v measure of unsupervisedXposHmm = {calculate_v_measure(xposGoldStandard,unsupervisedXposHmmPredictions)}\")\n",
        "print(f\"The variation of information of unsupervisedXposHmm = {calculate_variation_of_information(xposGoldStandard,unsupervisedXposHmmPredictions)}\\n\")\n",
        "\n",
        "# upos Kmeans\n",
        "print(f\"The Word Level Accuracy of uposKmeans = {calculate_accuracy(uposGoldStandard,uposKmeansPredictions)} %\")\n",
        "print(f\"The v measure of uposKmeans = {calculate_v_measure(uposGoldStandard,uposKmeansPredictions)}\")\n",
        "print(f\"The variation of information of uposKmeans = {calculate_variation_of_information(uposGoldStandard,uposKmeansPredictions)}\\n\")\n",
        "\n",
        "# xpos Kmeans\n",
        "print(f\"The Word Level Accuracy of xposKmeans = {calculate_accuracy(xposGoldStandard,xposKmeansPredictions)} %\")\n",
        "print(f\"The v measure of xposKmeans = {calculate_v_measure(xposGoldStandard,xposKmeansPredictions)}\")\n",
        "print(f\"The variation of information of xposKmeans = {calculate_variation_of_information(xposGoldStandard,xposKmeansPredictions)}\")"
      ],
      "metadata": {
        "id": "GiRR2NRkHm58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cbc1ab0-3651-4ee4-df67-79d0d35e6335"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Word Level Accuracy of supervisedUposHmm = 96.03485370957488 %\n",
            "The v measure of supervisedUposHmm = (0.9224970294144421, 0.9209958896710262, 0.9217458483604221)\n",
            "The variation of information of supervisedUposHmm = (0.5535692454803467, 0.07825415163957822)\n",
            "\n",
            "The Word Level Accuracy of supervisedxposHmm = 97.02966649404017 %\n",
            "The v measure of supervisedxposHmm = (0.9492879103476413, 0.947555596457507, 0.9484209623749003)\n",
            "The variation of information of supervisedxposHmm = (0.44820092764638986, 0.05157903762510059)\n",
            "\n",
            "The Word Level Accuracy of unsupervisedUposHmm = 47.64606937900778 %\n",
            "The v measure of unsupervisedUposHmm = (0.45771267530741094, 0.41971122908796255, 0.43788902618237335)\n",
            "The variation of information of unsupervisedUposHmm = (4.152998001030268, 0.5621109738176269)\n",
            "\n",
            "The Word Level Accuracy of unsupervisedXposHmm = 36.57123790035662 %\n",
            "The v measure of unsupervisedXposHmm = (0.5501194312375207, 0.47343491238617813, 0.5089045762002461)\n",
            "The variation of information of unsupervisedXposHmm = (4.608814484796389, 0.49109542379975507)\n",
            "\n",
            "The Word Level Accuracy of uposKmeans = 56.740643433667216 %\n",
            "The v measure of uposKmeans = (0.5618864629923878, 0.7357678583511614, 0.6371773941836508)\n",
            "The variation of information of uposKmeans = (2.2614830040850995, 0.3628226058163494)\n",
            "\n",
            "The Word Level Accuracy of xposKmeans = 47.02692973259735 %\n",
            "The v measure of xposKmeans = (0.6438184228482814, 0.6077932496115465, 0.6252873794532261)\n",
            "The variation of information of xposKmeans = (3.349536912210575, 0.37471262054677473)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing performance on short vs long sentences"
      ],
      "metadata": {
        "id": "18gDoLnXsDn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numWordsBeforeIt = 0\n",
        "hmmShortSentenceAcc=[]\n",
        "hmmLongSentenceAcc=[]\n",
        "kmeansShortSentenceAcc=[]\n",
        "kmeansLongSentenceAcc=[]\n",
        "for sentenceIdx in range(len(allUnlabeldSentences)):\n",
        "  sentence = allUnlabeldSentences[sentenceIdx]\n",
        "  goldStandard = uposGoldStandard[numWordsBeforeIt : numWordsBeforeIt + len(sentence)]\n",
        "  unsupervisedUposHmmOutput,hmmAcc = uposClusterToTag(goldStandard,unsupervisedUposHmmPredictions[numWordsBeforeIt : numWordsBeforeIt + len(sentence)])\n",
        "  uposKmeansOutput,kmeansAcc = uposClusterToTag(goldStandard,uposKmeansPredictions[numWordsBeforeIt : numWordsBeforeIt + len(sentence)])\n",
        "  goldStandard = [upos[i] for i in goldStandard]\n",
        "  numWordsBeforeIt += sentenceLength\n",
        "\n",
        "  if len(sentence)<10:\n",
        "    hmmShortSentenceAcc.append(hmmAcc)\n",
        "    kmeansShortSentenceAcc.append(kmeansAcc)\n",
        "  else:\n",
        "    hmmLongSentenceAcc.append(hmmAcc)\n",
        "    kmeansLongSentenceAcc.append(kmeansAcc)\n",
        "\n",
        "print(f\"HMM accuracy for short sentences = {np.mean(hmmShortSentenceAcc)} Vs Kmeans accuracy for short sentences = {np.mean(kmeansShortSentenceAcc)}\")\n",
        "print(f\"HMM accuracy for long sentences = {np.mean(hmmLongSentenceAcc)} Vs Kmeans accuracy for long sentences = {np.mean(kmeansLongSentenceAcc)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOQak_5FsCR3",
        "outputId": "d7a776d5-fab0-4dae-c7f7-b4beb786f544"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HMM accuracy for short sentences = 78.68333574564578 Vs Kmeans accuracy for short sentences = 74.59705939113235\n",
            "HMM accuracy for long sentences = 61.985082840133906 Vs Kmeans accuracy for long sentences = 63.75613207518351\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Looking closer at sentences to measure the models qualitatively."
      ],
      "metadata": {
        "id": "2o4VACgFSAQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a cluster to tag fucntion\n",
        "def uposClusterToTag(goldStandard,prediction):\n",
        "  numTags=17\n",
        "  confusionMatrix= np.zeros((numTags,numTags),dtype=int) # confusionMatrix[i][j] => if cluster i is mapped to tag with index j, how many correct tags will I get.\n",
        "  for i in range(len(prediction)):\n",
        "    confusionMatrix[prediction[i]][goldStandard[i]]+=1\n",
        "  _,clusterToTagIndex=linear_sum_assignment(-confusionMatrix) # Hungarian algorithm implementaion to find the optimal matching that maxmizes the accuracy.\n",
        "  prediction = [clusterToTagIndex[cluster] for cluster in prediction]\n",
        "  correct_words = sum([prediction[i]==goldStandard[i] for i in range(len(goldStandard))])\n",
        "  acc=100 * correct_words/len(goldStandard)\n",
        "  prediction = [upos[i] for i in prediction]\n",
        "  return prediction,acc"
      ],
      "metadata": {
        "id": "H1-75dS3E8Nl"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a random sentence goldStandard answer and the prediction of the models.\n",
        "# You can add some constrains to the code to see a particular difference between the model's performance\n",
        "# The code below gives us a sentence with length < 30 where the word \"open\" is tagged as \"VERB\" by the models and the gold standard.\n",
        "while 1:\n",
        "  sentenceIdx = randint(0,len(allUnlabeldSentences)-1)\n",
        "  # sentenceIdx = 39095\n",
        "  sentence = allUnlabeldSentences[sentenceIdx]\n",
        "  numWordsBeforeIt = sum([len(allUnlabeldSentences[i]) for i in range(sentenceIdx)])\n",
        "  sentenceLength = len(allUnlabeldSentences[sentenceIdx])\n",
        "\n",
        "  goldStandard = uposGoldStandard[numWordsBeforeIt : numWordsBeforeIt + sentenceLength]\n",
        "  # converting clusters to tags.\n",
        "  unsupervisedUposHmmOutput,hmmAcc = uposClusterToTag(goldStandard,unsupervisedUposHmmPredictions[numWordsBeforeIt : numWordsBeforeIt + sentenceLength])\n",
        "  uposKmeansOutput,kmeansAcc = uposClusterToTag(goldStandard,uposKmeansPredictions[numWordsBeforeIt : numWordsBeforeIt + sentenceLength])\n",
        "  goldStandard = [upos[i] for i in goldStandard]\n",
        "\n",
        "  if \"open\" in sentence and uposKmeansOutput[sentence.index(\"open\")]==\"VERB\" and len(sentence)<30 and unsupervisedUposHmmOutput[sentence.index(\"open\")]==\"VERB\" and goldStandard[sentence.index(\"open\")]==\"VERB\":\n",
        "    # printing statistics\n",
        "    print(f\"Sentence Index is {sentenceIdx}\")\n",
        "    print(*sentence)\n",
        "    print(f\"HMM Accuracy = {hmmAcc} %\")\n",
        "    print(f\"K-means Accuracy = {kmeansAcc} %\")\n",
        "    for i in range(len(sentence)):\n",
        "      print(sentence[i],goldStandard[i],unsupervisedUposHmmOutput[i],uposKmeansOutput[i])\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hflDhSEZSUE1",
        "outputId": "ae647b9f-9ce0-4298-91dd-eb606b52f76c"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Index is 30045\n",
            "The hotel is scheduled to open in 1992 .\n",
            "HMM Accuracy = 77.77777777777777 %\n",
            "K-means Accuracy = 66.66666666666667 %\n",
            "The DET DET VERB\n",
            "hotel NOUN NOUN VERB\n",
            "is AUX PART AUX\n",
            "scheduled VERB PROPN VERB\n",
            "to PART PART PART\n",
            "open VERB VERB VERB\n",
            "in ADP ADP ADP\n",
            "1992 NUM NUM VERB\n",
            ". PUNCT PUNCT PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The code below to check which words have different tags, so that I can invstigate the performance of the models when the word is used in different contexts\n",
        "wordToTag={word:set() for word in vocab}\n",
        "for s in uposCorpus:\n",
        "  for w in s:\n",
        "    wordToTag[w[\"word\"]].add(w[\"tag\"])"
      ],
      "metadata": {
        "id": "6VgZsW6Pr06Z"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in vocab:\n",
        "  if len(wordToTag[word])>4:\n",
        "    print(word)\n",
        "    print(wordToTag[word])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRYaRtLgs4jq",
        "outputId": "35d4c6d1-e62c-48aa-8593-61b7d4b06d47"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'s\n",
            "{'VERB', 'PART', 'PROPN', 'AUX', 'PRON', 'NOUN'}\n",
            "back\n",
            "{'ADJ', 'ADP', 'VERB', 'ADV', 'NOUN'}\n",
            "down\n",
            "{'ADJ', 'ADP', 'VERB', 'ADV', 'NOUN'}\n",
            "open\n",
            "{'ADJ', 'ADP', 'VERB', 'ADV', 'NOUN'}\n",
            "that\n",
            "{'ADP', 'VERB', 'ADV', 'SCONJ', 'PRON', 'DET', 'NOUN'}\n",
            "the\n",
            "{'ADJ', 'PROPN', 'VERB', 'DET', 'NOUN'}\n",
            "vs.\n",
            "{'ADJ', 'X', 'CONJ', 'ADP', 'NOUN'}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}